{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee82fde9",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "318a5c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "720f2f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-03 10:35:43,876 INFO __main__: Logging configured: INFO level to stdout and auto_report.log\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s %(name)s: %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "        logging.FileHandler(\"auto_report.log\", encoding=\"utf-8\")\n",
    "    ]\n",
    ")\n",
    "logger.info(\"Logging configured: INFO level to stdout and auto_report.log\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db50b27d",
   "metadata": {},
   "source": [
    "# Read excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd242fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.read_excel(r\"C:\\Users\\Massyle\\Documents\\auto_report\\data\\Enq2025_Calculs TCD Filères.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fc3c80",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feb6ddd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "without_numbers = full_data.copy()\n",
    "without_numbers.columns = (\n",
    "    without_numbers.columns\n",
    "    .str.replace(r\"^\\s*\\d+\\.?\\s*\", \"\", regex=True)\n",
    "    .str.strip()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5f5713",
   "metadata": {},
   "source": [
    "# Interval year\n",
    "[max-2;max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b1523c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-03 10:35:50,121 INFO __main__: Max year: 2024\n"
     ]
    }
   ],
   "source": [
    "max_year = pd.to_numeric(without_numbers['AnneeDiplomeVerifiee'], errors='coerce').max()\n",
    "logger.info(f\"Max year: {max_year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cb293d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = pd.to_numeric(without_numbers['AnneeDiplomeVerifiee'], errors='coerce')\n",
    "min_year = max_year - 2\n",
    "filtered = without_numbers[(years >= min_year) & (years <= max_year)].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "987125c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-03 10:35:50,163 INFO __main__: Total lines: 1449\n"
     ]
    }
   ],
   "source": [
    "total_lines = filtered['AnneeDiplomeVerifiee'].count()\n",
    "logger.info(f\"Total lines: {total_lines}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e823907",
   "metadata": {},
   "source": [
    "professionnalisation fusionne avec initial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02991500",
   "metadata": {},
   "source": [
    "# TCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5d5c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel summary written: C:\\Users\\Massyle\\Documents\\auto_report\\Enq_TCD_20251103.xlsx\n",
      "DataFrames saved for reuse: C:\\Users\\Massyle\\Documents\\auto_report\\Enq_TCD_20251103.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from openpyxl.utils import get_column_letter\n",
    "import pickle\n",
    "\n",
    "# -------------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------------\n",
    "summary_columns = [\n",
    "    \"Situation\",\n",
    "    \"EmploiLieuRegionEtranger\",\n",
    "    \"EmploiContrat\",\n",
    "    \"EmploiFranceCadre\",\n",
    "    \"EmploiEntrepriseTaille\",\n",
    "    # \"Calcul_Euros_EmploiSalaireBrutAnnuelAP\",\n",
    "    \"1erEmploiLapsPourTrouverApresDiplome\",\n",
    "    \"EmploiCommentTrouve\",\n",
    "    \"EmploiSecteur\",\n",
    "    \"EmploiService\"\n",
    "]\n",
    "\n",
    "GENDER_COL = \"IdentiteSexeVerifie\"\n",
    "YEAR_COL = \"AnneeDiplomeVerifiee\"\n",
    "\n",
    "assert 'filtered' in globals(), \"Expected a DataFrame named 'filtered' to be defined earlier.\"\n",
    "\n",
    "out_dir = r\"C:\\Users\\Massyle\\Documents\\auto_report\"\n",
    "filename = f\"Enq_TCD_{datetime.now().strftime('%Y%m%d')}.xlsx\"\n",
    "out_path = os.path.join(out_dir, filename)\n",
    "\n",
    "invalid = set('[]:*?/\\\\')\n",
    "used_names = set()\n",
    "\n",
    "def safe_sheet_name(name: str) -> str:\n",
    "    cleaned = ''.join('_' if ch in invalid else ch for ch in name)\n",
    "    base = cleaned[:31] or \"sheet\"\n",
    "    candidate = base\n",
    "    i = 1\n",
    "    while candidate in used_names:\n",
    "        suffix = f\"_{i}\"\n",
    "        candidate = (base[:31 - len(suffix)]) + suffix\n",
    "        i += 1\n",
    "    used_names.add(candidate)\n",
    "    return candidate\n",
    "\n",
    "# -------------------------------\n",
    "# 1. BUILD DATAFRAMES IN MEMORY\n",
    "# -------------------------------\n",
    "sheets_dict = {}\n",
    "\n",
    "for col in summary_columns:\n",
    "    sheet_name = safe_sheet_name(col)\n",
    "\n",
    "    if col not in filtered.columns:\n",
    "        sheets_dict[sheet_name] = pd.DataFrame({\"message\": [f\"Column '{col}' not found in DataFrame\"]})\n",
    "        continue\n",
    "\n",
    "    # Base grouping logic\n",
    "    if {GENDER_COL, YEAR_COL}.issubset(filtered.columns):\n",
    "        temp = filtered[[col, GENDER_COL, YEAR_COL]].dropna(subset=[col])\n",
    "\n",
    "        # Count by Year + Gender\n",
    "        grouped = temp.groupby([YEAR_COL, GENDER_COL, col]).size().reset_index(name=\"Count\")\n",
    "\n",
    "        # Pivot to have one column per (Year, Gender)\n",
    "        pivot = grouped.pivot_table(\n",
    "            index=col,\n",
    "            columns=[YEAR_COL, GENDER_COL],\n",
    "            values=\"Count\",\n",
    "            fill_value=0\n",
    "        )\n",
    "\n",
    "        # Add per-year total (across genders)\n",
    "        years = pivot.columns.get_level_values(0).unique()\n",
    "        for y in years:\n",
    "            pivot[(y, \"Total\")] = pivot.loc[:, y].sum(axis=1)\n",
    "\n",
    "        # Sort columns by year then gender (H, F, Total)\n",
    "        pivot = pivot.sort_index(axis=1, level=[0, 1])\n",
    "\n",
    "        sheets_dict[sheet_name] = pivot\n",
    "\n",
    "    else:\n",
    "        # Simple fallback: just counts for that column\n",
    "        counts = filtered[col].value_counts(dropna=False).rename_axis(col).reset_index(name=\"Count\")\n",
    "        sheets_dict[sheet_name] = counts\n",
    "\n",
    "# -------------------------------\n",
    "# 2. WRITE ALL SHEETS TO EXCEL\n",
    "# -------------------------------\n",
    "with pd.ExcelWriter(out_path, engine=\"openpyxl\") as writer:\n",
    "    for sheet_name, df in sheets_dict.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name)\n",
    "\n",
    "        ws = writer.book[sheet_name]\n",
    "\n",
    "        # Auto-fit columns\n",
    "        df_reset = df.reset_index() if df.index.names != [None] else df\n",
    "        for idx, column in enumerate(df_reset.columns, start=1):\n",
    "            column_letter = get_column_letter(idx)\n",
    "            values = df_reset[column].astype(str).tolist()\n",
    "            max_len = max([len(column)] + [len(v) for v in values]) if values else len(column)\n",
    "            ws.column_dimensions[column_letter].width = max_len + 2\n",
    "\n",
    "print(f\"Excel summary written: {out_path}\")\n",
    "\n",
    "# Sauvegarde du dictionnaire complet contenant tous les DataFrames\n",
    "pickle_path = out_path.replace(\".xlsx\", \".pkl\")\n",
    "with open(pickle_path, \"wb\") as f:\n",
    "    pickle.dump(sheets_dict, f)\n",
    "\n",
    "print(f\"DataFrames saved for reuse: {pickle_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fd2cbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-03 10:12:36,295 INFO __main__: Loading data from C:\\Users\\Massyle\\Documents\\auto_report\\Enq_TCD_20251103_Ecole_Branche_abr.pkl\n",
      "2025-11-03 10:12:36,311 INFO __main__: Saving aggregated data to C:\\Users\\Massyle\\Documents\\auto_report\\Enq_TCD_20251103_aggregated.pkl\n",
      "2025-11-03 10:12:36,317 INFO __main__: Aggregated data saved successfully!\n",
      "\n",
      "Aggregated data saved to: C:\\Users\\Massyle\\Documents\\auto_report\\Enq_TCD_20251103_aggregated.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def aggregate_data(pickle_path):\n",
    "    \"\"\"\n",
    "    Aggregate specific categories in the data before converting to percentages.\n",
    "    \n",
    "    Args:\n",
    "        pickle_path (str): Path to the original pickle file\n",
    "        \n",
    "    Returns:\n",
    "        dict: Aggregated data dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the original data\n",
    "    logger.info(f\"Loading data from {pickle_path}\")\n",
    "    with open(pickle_path, \"rb\") as f:\n",
    "        sheets_dict = pickle.load(f)\n",
    "    \n",
    "    # Create a copy for aggregation\n",
    "    aggregated_sheets = sheets_dict.copy()\n",
    "    \n",
    "    # 1. Aggregate EmploiLieuRegionEtranger\n",
    "    if 'EmploiLieuRegionEtranger' in aggregated_sheets:\n",
    "        logger.info(\"Aggregating EmploiLieuRegionEtranger...\")\n",
    "        df_region = aggregated_sheets['EmploiLieuRegionEtranger'].copy()\n",
    "        \n",
    "        # Define regions to keep as is\n",
    "        regions_to_keep = ['Île-de-France', 'Étranger']\n",
    "        \n",
    "        # Get all other regions (to be summed into 'Province')\n",
    "        other_regions = [region for region in df_region.index if region not in regions_to_keep]\n",
    "        \n",
    "        if other_regions:\n",
    "            # Sum all other regions into 'Province'\n",
    "            province_data = df_region.loc[other_regions].sum()\n",
    "            \n",
    "            # Create new dataframe with aggregated data\n",
    "            # Keep the original regions that we want to preserve\n",
    "            kept_data = df_region.loc[regions_to_keep]\n",
    "            \n",
    "            # Add the new 'Province' row\n",
    "            province_df = pd.DataFrame([province_data], index=['Province'])\n",
    "            \n",
    "            # Combine kept regions and province\n",
    "            aggregated_region = pd.concat([kept_data, province_df])\n",
    "            \n",
    "            # Sort index for better readability\n",
    "            aggregated_region = aggregated_region.sort_index()\n",
    "            \n",
    "            aggregated_sheets['EmploiLieuRegionEtranger'] = aggregated_region\n",
    "            \n",
    "            logger.info(f\"Regions aggregated: {len(other_regions)} regions combined into 'Province'\")\n",
    "            logger.info(f\"Final regions: {list(aggregated_region.index)}\")\n",
    "    \n",
    "    # 2. Aggregate EmploiEntrepriseTaille\n",
    "    if 'EmploiEntrepriseTaille' in aggregated_sheets:\n",
    "        logger.info(\"Aggregating EmploiEntrepriseTaille...\")\n",
    "        df_taille = aggregated_sheets['EmploiEntrepriseTaille'].copy()\n",
    "        \n",
    "        # Define categories to sum into 'Moins de 10'\n",
    "        categories_to_sum = ['0', 'De 1 à 9']\n",
    "        \n",
    "        # Check if both categories exist\n",
    "        existing_categories = [cat for cat in categories_to_sum if cat in df_taille.index]\n",
    "        \n",
    "        if len(existing_categories) > 0:\n",
    "            # Sum the specified categories\n",
    "            moins_de_10_data = df_taille.loc[existing_categories].sum()\n",
    "            \n",
    "            # Create new dataframe without the categories we're aggregating\n",
    "            other_categories = [cat for cat in df_taille.index if cat not in categories_to_sum]\n",
    "            kept_data = df_taille.loc[other_categories]\n",
    "            \n",
    "            # Add the new 'Moins de 10' row\n",
    "            moins_de_10_df = pd.DataFrame([moins_de_10_data], index=['Moins de 10'])\n",
    "            \n",
    "            # Combine kept categories and new aggregated category\n",
    "            aggregated_taille = pd.concat([kept_data, moins_de_10_df])\n",
    "            \n",
    "            # Sort index for better readability\n",
    "            aggregated_taille = aggregated_taille.sort_index()\n",
    "            \n",
    "            aggregated_sheets['EmploiEntrepriseTaille'] = aggregated_taille\n",
    "            \n",
    "            logger.info(f\"Categories aggregated: {existing_categories} combined into 'Moins de 10'\")\n",
    "            logger.info(f\"Final categories: {list(aggregated_taille.index)}\")\n",
    "    \n",
    "    return aggregated_sheets\n",
    "\n",
    "def save_aggregated_data(aggregated_sheets, output_path):\n",
    "    \"\"\"\n",
    "    Save the aggregated data to a new pickle file.\n",
    "    \n",
    "    Args:\n",
    "        aggregated_sheets (dict): Aggregated data dictionary\n",
    "        output_path (str): Path for the output pickle file\n",
    "    \"\"\"\n",
    "    logger.info(f\"Saving aggregated data to {output_path}\")\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        pickle.dump(aggregated_sheets, f)\n",
    "    \n",
    "    logger.info(\"Aggregated data saved successfully!\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the aggregation process.\"\"\"\n",
    "    \n",
    "    # Input and output paths\n",
    "    input_pickle = r\"C:\\Users\\Massyle\\Documents\\auto_report\\Enq_TCD_20251103_Ecole_Branche_abr.pkl\"\n",
    "    output_pickle = r\"C:\\Users\\Massyle\\Documents\\auto_report\\Enq_TCD_20251103_aggregated.pkl\"\n",
    "    \n",
    "    try:\n",
    "        # Perform aggregation\n",
    "        aggregated_data = aggregate_data(input_pickle)\n",
    "        \n",
    "        # Save aggregated data\n",
    "        save_aggregated_data(aggregated_data, output_pickle)\n",
    "        \n",
    "        # Show EmploiLieuRegionEtranger changes\n",
    "        if 'EmploiLieuRegionEtranger' in aggregated_data:\n",
    "            print(\"\\nEmploiLieuRegionEtranger:\")\n",
    "            print(\"Final regions:\", list(aggregated_data['EmploiLieuRegionEtranger'].index))\n",
    "        \n",
    "        # Show EmploiEntrepriseTaille changes\n",
    "        if 'EmploiEntrepriseTaille' in aggregated_data:\n",
    "            print(\"\\nEmploiEntrepriseTaille:\")\n",
    "            print(\"Final categories:\", list(aggregated_data['EmploiEntrepriseTaille'].index))\n",
    "        \n",
    "        print(f\"\\nAggregated data saved to: {output_pickle}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during aggregation: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ee56441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-28 07:26:11,823 INFO __main__: Loading aggregated data from C:\\Users\\Massyle\\Documents\\auto_report\\Enq_TCD_20251028_aggregated.pkl\n",
      "2025-10-28 07:26:11,884 INFO __main__: Converting to percentages...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-28 07:26:13,270 INFO __main__: Saving percentage data to C:\\Users\\Massyle\\Documents\\auto_report\\Enq_TCD_20251028_aggregated_percent.xlsx\n",
      "2025-10-28 07:26:15,867 INFO __main__: Percentage Excel file saved successfully!\n",
      "2025-10-28 07:26:15,872 INFO __main__: Saving percentage data to C:\\Users\\Massyle\\Documents\\auto_report\\Enq_TCD_20251028_aggregated_percent.pkl\n",
      "\n",
      "============================================================\n",
      "PERCENTAGE CONVERSION SUMMARY\n",
      "============================================================\n",
      "Percentage Excel saved to: C:\\Users\\Massyle\\Documents\\auto_report\\Enq_TCD_20251028_aggregated_percent.xlsx\n",
      "Percentage pickle saved to: C:\\Users\\Massyle\\Documents\\auto_report\\Enq_TCD_20251028_aggregated_percent.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def convert_to_percentages(sheets_dict):\n",
    "    \"\"\"\n",
    "    Convert aggregated data to percentages.\n",
    "    \n",
    "    Args:\n",
    "        sheets_dict (dict): Dictionary containing aggregated DataFrames\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing percentage DataFrames\n",
    "    \"\"\"\n",
    "    percent_sheets = {}\n",
    "    \n",
    "    for name, df in sheets_dict.items():\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Find numeric columns (handles MultiIndex as well)\n",
    "        numeric_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "        \n",
    "        if len(numeric_cols) == 0:\n",
    "            # No numeric columns -> copy as is\n",
    "            percent_sheets[name] = df\n",
    "            continue\n",
    "        \n",
    "        # Sum by column\n",
    "        col_sums = df[numeric_cols].sum(axis=0)\n",
    "        \n",
    "        # Avoid division by zero: if sum == 0, set 0 for all values in that column\n",
    "        # Calculate %: value / col_sum * 100\n",
    "        # .div handles alignment on column index for MultiIndex as well\n",
    "        percent_numeric = df[numeric_cols].div(col_sums).multiply(100)\n",
    "        \n",
    "        # Columns with sum 0 -> replace NaN/inf with 0\n",
    "        zero_cols = col_sums[col_sums == 0].index\n",
    "        if len(zero_cols) > 0:\n",
    "            percent_numeric.loc[:, zero_cols] = 0.0\n",
    "        \n",
    "        # Re-inject non-numeric columns in the same order as original\n",
    "        # Build final DataFrame respecting original column order\n",
    "        result = pd.DataFrame(index=df.index)\n",
    "        for col in df.columns:\n",
    "            if col in numeric_cols:\n",
    "                result[col] = percent_numeric[col]\n",
    "            else:\n",
    "                result[col] = df[col]\n",
    "        \n",
    "        # Keep same dtype of index / names etc.\n",
    "        percent_sheets[name] = result\n",
    "    \n",
    "    return percent_sheets\n",
    "\n",
    "def save_percentages_to_excel(percent_sheets, output_path):\n",
    "    \"\"\"\n",
    "    Save percentage data to Excel file.\n",
    "    \n",
    "    Args:\n",
    "        percent_sheets (dict): Dictionary containing percentage DataFrames\n",
    "        output_path (str): Path for the output Excel file\n",
    "    \"\"\"\n",
    "    from openpyxl.utils import get_column_letter\n",
    "    \n",
    "    logger.info(f\"Saving percentage data to {output_path}\")\n",
    "    \n",
    "    with pd.ExcelWriter(output_path, engine=\"openpyxl\") as writer:\n",
    "        for name, df in percent_sheets.items():\n",
    "            # By default, keep the index (like for the original)\n",
    "            df.to_excel(writer, sheet_name=name)\n",
    "            \n",
    "            # Auto-fit columns\n",
    "            ws = writer.book[name]\n",
    "            df_reset = df.reset_index() if df.index.names != [None] else df\n",
    "            for idx, column in enumerate(df_reset.columns, start=1):\n",
    "                column_letter = get_column_letter(idx)\n",
    "                values = df_reset[column].astype(str).tolist()\n",
    "                max_len = max([len(column)] + [len(v) for v in values]) if values else len(column)\n",
    "                ws.column_dimensions[column_letter].width = max_len + 2\n",
    "    \n",
    "    logger.info(\"Percentage Excel file saved successfully!\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to convert aggregated data to percentages.\"\"\"\n",
    "    \n",
    "    # Input and output paths\n",
    "    input_pickle = r\"C:\\Users\\Massyle\\Documents\\auto_report\\Enq_TCD_20251028_aggregated.pkl\"\n",
    "    output_excel = r\"C:\\Users\\Massyle\\Documents\\auto_report\\Enq_TCD_20251028_aggregated_percent.xlsx\"\n",
    "    output_pickle = r\"C:\\Users\\Massyle\\Documents\\auto_report\\Enq_TCD_20251028_aggregated_percent.pkl\"\n",
    "    \n",
    "    try:\n",
    "        # Load aggregated data\n",
    "        logger.info(f\"Loading aggregated data from {input_pickle}\")\n",
    "        with open(input_pickle, \"rb\") as f:\n",
    "            aggregated_sheets = pickle.load(f)\n",
    "        \n",
    "        # Convert to percentages\n",
    "        logger.info(\"Converting to percentages...\")\n",
    "        percent_sheets = convert_to_percentages(aggregated_sheets)\n",
    "        \n",
    "        # Save percentages to Excel\n",
    "        save_percentages_to_excel(percent_sheets, output_excel)\n",
    "        \n",
    "        # Save percentages to pickle\n",
    "        logger.info(f\"Saving percentage data to {output_pickle}\")\n",
    "        with open(output_pickle, \"wb\") as f:\n",
    "            pickle.dump(percent_sheets, f)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PERCENTAGE CONVERSION SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Percentage Excel saved to: {output_excel}\")\n",
    "        print(f\"Percentage pickle saved to: {output_pickle}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during percentage conversion: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cd7546d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Excel written: C:\\Users\\Massyle\\Documents\\auto_report\\Enq_TCD_20251028_percent.xlsx\n"
     ]
    }
   ],
   "source": [
    "# paths (doit exister pickle_path défini par ton script précédent)\n",
    "# ex: pickle_path = r\"C:\\Users\\Massyle\\Documents\\auto_report\\Enq_TCD_20251020_121233.pkl\"\n",
    "with open(pickle_path, \"rb\") as f:\n",
    "    sheets_dict = pickle.load(f)\n",
    "\n",
    "percent_sheets = {}\n",
    "\n",
    "for name, df in sheets_dict.items():\n",
    "    df = df.copy()\n",
    "\n",
    "    # trouver colonnes numériques (gère aussi MultiIndex)\n",
    "    numeric_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "\n",
    "    if len(numeric_cols) == 0:\n",
    "        # pas de numériques -> on copie tel quel\n",
    "        percent_sheets[name] = df\n",
    "        continue\n",
    "\n",
    "    # somme par colonne\n",
    "    col_sums = df[numeric_cols].sum(axis=0)\n",
    "\n",
    "    # éviter division par zéro : si somme == 0, on met 0 pour toutes les valeurs de la colonne\n",
    "    # calc % : valeur / col_sum * 100\n",
    "    # .div accepte l'alignement sur l'index des colonnes pour MultiIndex aussi\n",
    "    percent_numeric = df[numeric_cols].div(col_sums).multiply(100)\n",
    "\n",
    "    # colonnes avec somme 0 -> remplacer NaN/inf par 0\n",
    "    zero_cols = col_sums[col_sums == 0].index\n",
    "    if len(zero_cols) > 0:\n",
    "        percent_numeric.loc[:, zero_cols] = 0.0\n",
    "\n",
    "    # réinjecter les colonnes non numériques dans le même ordre que l'original\n",
    "    # construire DataFrame final en respectant l'ordre original des colonnes\n",
    "    result = pd.DataFrame(index=df.index)\n",
    "    for col in df.columns:\n",
    "        if col in numeric_cols:\n",
    "            result[col] = percent_numeric[col]\n",
    "        else:\n",
    "            result[col] = df[col]\n",
    "\n",
    "    # conserver le même dtype d'index / noms etc.\n",
    "    percent_sheets[name] = result\n",
    "\n",
    "# écrire le fichier percent\n",
    "percent_out_path = pickle_path.replace(\".pkl\", \"_percent.xlsx\")\n",
    "with pd.ExcelWriter(percent_out_path, engine=\"openpyxl\") as writer:\n",
    "    for name, df in percent_sheets.items():\n",
    "        # par défaut on conserve l'index (comme pour l'original)\n",
    "        df.to_excel(writer, sheet_name=name)\n",
    "\n",
    "print(f\"Percent Excel written: {percent_out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feabdc3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e6c1479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 45 sheets.\n",
      "First 10 keys:\n",
      " - IM_Situation\n",
      " - IM_EmploiLieuRegionEtranger\n",
      " - IM_EmploiContrat\n",
      " - IM_EmploiFranceCadre\n",
      " - IM_EmploiEntrepriseTaille\n",
      " - IM_1erEmploiLapsPourTrouverApre\n",
      " - IM_EmploiCommentTrouve\n",
      " - IM_EmploiSecteur\n",
      " - IM_EmploiService\n",
      " - GI_Situation\n",
      "\n",
      "=== Search for: EmploiLieuRegionEtranger ===\n",
      "Index-name matches (recommended):\n",
      " * IM_EmploiLieuRegionEtranger\n",
      " * GI_EmploiLieuRegionEtranger\n",
      " * GU_EmploiLieuRegionEtranger\n",
      " * GB_EmploiLieuRegionEtranger\n",
      " * GP_EmploiLieuRegionEtranger\n",
      "Loose key contains:\n",
      " - IM_EmploiLieuRegionEtranger\n",
      " - GI_EmploiLieuRegionEtranger\n",
      " - GU_EmploiLieuRegionEtranger\n",
      " - GB_EmploiLieuRegionEtranger\n",
      " - GP_EmploiLieuRegionEtranger\n",
      "Close key matches:\n",
      " ~ IM_EmploiLieuRegionEtranger\n",
      " ~ GU_EmploiLieuRegionEtranger\n",
      " ~ GP_EmploiLieuRegionEtranger\n",
      " ~ GI_EmploiLieuRegionEtranger\n",
      " ~ GB_EmploiLieuRegionEtranger\n",
      "\n",
      "=== Search for: EmploiEntrepriseTaille ===\n",
      "Index-name matches (recommended):\n",
      " * IM_EmploiEntrepriseTaille\n",
      " * GI_EmploiEntrepriseTaille\n",
      " * GU_EmploiEntrepriseTaille\n",
      " * GB_EmploiEntrepriseTaille\n",
      " * GP_EmploiEntrepriseTaille\n",
      "Loose key contains:\n",
      " - IM_EmploiEntrepriseTaille\n",
      " - GI_EmploiEntrepriseTaille\n",
      " - GU_EmploiEntrepriseTaille\n",
      " - GB_EmploiEntrepriseTaille\n",
      " - GP_EmploiEntrepriseTaille\n",
      "Close key matches:\n",
      " ~ IM_EmploiEntrepriseTaille\n",
      " ~ GU_EmploiEntrepriseTaille\n",
      " ~ GP_EmploiEntrepriseTaille\n",
      " ~ GI_EmploiEntrepriseTaille\n",
      " ~ GB_EmploiEntrepriseTaille\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from difflib import get_close_matches\n",
    "\n",
    "# Use the same pickle_path variable already defined above or set it here\n",
    "# pickle_path = r\"C:\\Users\\Massyle\\Documents\\auto_report\\Enq_TCD_20251103_aggregated.pkl\"\n",
    "\n",
    "with open(pickle_path, \"rb\") as f:\n",
    "    sheets_dict = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded {len(sheets_dict)} sheets.\")\n",
    "\n",
    "# Show a few keys to inspect naming\n",
    "all_keys = list(sheets_dict.keys())\n",
    "print(\"First 10 keys:\")\n",
    "for k in all_keys[:10]:\n",
    "    print(\" -\", k)\n",
    "\n",
    "\n",
    "def find_by_index_name(d: dict, target: str):\n",
    "    matches = []\n",
    "    for key, df in d.items():\n",
    "        idx_name = getattr(getattr(df, \"index\", None), \"name\", None)\n",
    "        if idx_name == target:\n",
    "            matches.append(key)\n",
    "    return matches\n",
    "\n",
    "\n",
    "def find_by_loose_key(d: dict, target: str):\n",
    "    t = target.lower()\n",
    "    return [k for k in d.keys() if t in k.lower()]\n",
    "\n",
    "\n",
    "def debug_find(target: str):\n",
    "    exact = find_by_index_name(sheets_dict, target)\n",
    "    loose = find_by_loose_key(sheets_dict, target)\n",
    "    close = get_close_matches(target, all_keys, n=5, cutoff=0.6)\n",
    "\n",
    "    print(\"\\n=== Search for:\", target, \"===\")\n",
    "    if exact:\n",
    "        print(\"Index-name matches (recommended):\")\n",
    "        for k in exact:\n",
    "            print(\" *\", k)\n",
    "    else:\n",
    "        print(\"No index-name matches.\")\n",
    "\n",
    "    if loose:\n",
    "        print(\"Loose key contains:\")\n",
    "        for k in loose:\n",
    "            print(\" -\", k)\n",
    "\n",
    "    if close:\n",
    "        print(\"Close key matches:\")\n",
    "        for k in close:\n",
    "            print(\" ~\", k)\n",
    "\n",
    "# Targets to verify\n",
    "for col in [\"EmploiLieuRegionEtranger\", \"EmploiEntrepriseTaille\"]:\n",
    "    debug_find(col)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5128b795",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
